{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax  regression model\n",
    "\n",
    "The following notebook describes the creation of a softmax  regression model for the classification of german traffic signs, using tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eduardo\\Anaconda\\Miniconda3\\envs\\dl\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from PIL import Image\n",
    "import glob\n",
    "import inspect\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the train and test images\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "ruta_train = os.sep.join([current_dir,'..','images','train'])\n",
    "ruta_test = os.sep.join([current_dir,'..','images','test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test images\n",
    "\n",
    "train_images = []\n",
    "test_images = []\n",
    "\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "for filename in glob.iglob(ruta_train + os.sep + '*.ppm'):\n",
    "    label = int(filename.split(os.sep)[-1].split('_')[0])\n",
    "    y_train.append(label)\n",
    "    train_images.append(Image.open(filename))\n",
    "\n",
    "for filename in glob.iglob(ruta_test + os.sep + '*.ppm'):\n",
    "    label = int(filename.split(os.sep)[-1].split('_')[0])\n",
    "    y_test.append(label)\n",
    "    test_images.append(Image.open(filename))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# Resizes an image to a defined size, and then reshapes into a 1xn array\n",
    "def process_img(img,size):\n",
    "    out = img.resize(size, Image.ANTIALIAS)\n",
    "    out = np.asarray(out)\n",
    "    out = out.reshape(1, -1)\n",
    "    return out\n",
    "\n",
    "# Applies the process_img function to the train and test sets of images, and return train and test arrays\n",
    "def formar_set(train_images,test_images,im_size):\n",
    "    X_train = [process_img(img, im_size) for img in train_images]\n",
    "    X_train = np.vstack(X_train)\n",
    "\n",
    "    X_test = [process_img(img, im_size) for img in test_images]\n",
    "    X_test = np.vstack(X_test)\n",
    "    \n",
    "    return X_train,X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "im_size = (25,25)\n",
    "learning_rate = 0.0002\n",
    "batch_size = 500\n",
    "training_iteration = 10000 # max number of iterations\n",
    "termination_margin = 0.001/100  # If the cost function varies less than this between iterations, the training process stops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and normalize training and test feature sets\n",
    "\n",
    "X_train,X_test = formar_set(train_images,test_images,im_size)\n",
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255\n",
    "\n",
    "# Convert class vectors (y_train and y_test) to binary class matrices\n",
    "\n",
    "Y_train = np.array(pd.get_dummies(pd.Series(y_train)))\n",
    "Y_test = np.array(pd.get_dummies(pd.Series(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Create tf model\n",
    "\n",
    "# size_x = X_train.shape[1]\n",
    "# size_y = Y_train.shape[1]\n",
    "\n",
    "# # TF graph input\n",
    "# x = tf.placeholder(\"float\", [None, size_x], name='x') \n",
    "# y = tf.placeholder(\"float\", [None, size_y], name='y') \n",
    "\n",
    "# # Set model weights\n",
    "# W = tf.Variable(tf.zeros([size_x, size_y]), name='W')\n",
    "# b = tf.Variable(tf.zeros([size_y]), name='b')\n",
    "\n",
    "# # Construct a linear model\n",
    "# model = tf.nn.softmax(tf.matmul(x, W) + b, name='model') # Softmax\n",
    "\n",
    "# # Define cross_entropy as the cost function\n",
    "# cost_function = -tf.reduce_sum(y*tf.log(model))\n",
    "                \n",
    "# # Define Gradient Descent as the model optimizer\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n",
    "\n",
    "# # Initializing the variables\n",
    "# init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\eduardo\\Desktop\\Reto Kiwi\\German traffic signs detector\\reports\\..\\models\\model2\\model2-1000\n",
      "[15 16 16 17 17 17 17 17 17 18 26 18 18 18]\n",
      "[15, 16, 16, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18]\n"
     ]
    }
   ],
   "source": [
    "x_s = X_test[166:180]\n",
    "# x_s = x_s.reshape(1,-1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    new_saver = tf.train.import_meta_graph('C:/Users/eduardo/Desktop/Reto Kiwi/German traffic signs detector/models/model2/model2-1000.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('C:/Users/eduardo/Desktop/Reto Kiwi/German traffic signs detector/models/model2/'))\n",
    "    pred = tf.argmax(model,1)\n",
    "    print(sess.run(pred,feed_dict={x:x_s}))\n",
    "    \n",
    "#     graph = tf.get_default_graph()\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     x = graph.get_tensor_by_name(\"x:0\")\n",
    "#     W = graph.get_tensor_by_name(\"W:0\")\n",
    "#     b = graph.get_tensor_by_name(\"b:0\")\n",
    "    \n",
    "#     model = graph.get_tensor_by_name(\"model:0\")\n",
    "# #     y = graph.get_tensor_by_name(\"y\")\n",
    "#     pred = tf.argmax(model,1)\n",
    "       \n",
    "#     print(sess.run(pred,feed_dict={x: x_s}))\n",
    "print(y_test[166:180])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
